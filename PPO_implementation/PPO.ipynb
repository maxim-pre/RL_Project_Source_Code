{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2yTXAw7Xx-W",
        "outputId": "8d9ed7a1-a5f6-4663-e0e2-8378477c50bb"
      },
      "outputs": [],
      "source": [
        "! pip install swig\n",
        "! pip install box2d-py\n",
        "! pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8Hdj53q6jmZ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/BipedalWalker_Project\"\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "print(f\"Saving all models to: {DRIVE_FOLDER}\")\n",
        "\n",
        "def show_video(video_folder=\"videos\"):\n",
        "    mp4list = glob.glob(f'{video_folder}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = max(mp4list, key=os.path.getctime)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"No video found yet.\")\n",
        "\n",
        "# Model architecture\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 1), std=1.0),\n",
        "        )\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, act_dim), std=0.01),\n",
        "        )\n",
        "        self.actor_logstd = nn.Parameter(torch.ones(1, act_dim) * -0.5)\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        action_mean = self.actor_mean(x)\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_logstd)\n",
        "        dist = Normal(action_mean, action_std)\n",
        "\n",
        "        if action is None:\n",
        "            action = dist.sample()\n",
        "\n",
        "        log_prob = dist.log_prob(action).sum(1)\n",
        "        entropy = dist.entropy().sum(1)\n",
        "        value = self.critic(x)\n",
        "        return action, log_prob, entropy, value.squeeze(1)\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self, size, obs_dim, act_dim, device):\n",
        "        self.size = size\n",
        "        self.device = device\n",
        "        self.obs = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros((size, act_dim), dtype=np.float32)\n",
        "        self.log_probs = np.zeros(size, dtype=np.float32)\n",
        "        self.rewards = np.zeros(size, dtype=np.float32)\n",
        "        self.dones = np.zeros(size, dtype=np.float32)\n",
        "        self.values = np.zeros(size, dtype=np.float32)\n",
        "        self.advantages = np.zeros(size, dtype=np.float32)\n",
        "        self.returns = np.zeros(size, dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "        self.path_start_idx = 0\n",
        "\n",
        "    def store(self, obs, action, log_prob, reward, done, value):\n",
        "        assert self.ptr < self.size\n",
        "        self.obs[self.ptr] = obs\n",
        "        self.actions[self.ptr] = action\n",
        "        self.log_probs[self.ptr] = log_prob\n",
        "        self.rewards[self.ptr] = reward\n",
        "        self.dones[self.ptr] = done\n",
        "        self.values[self.ptr] = value\n",
        "        self.ptr += 1\n",
        "\n",
        "    def finish_path(self, last_value, gamma, lam):\n",
        "        path_slice = slice(self.path_start_idx, self.ptr)\n",
        "        rewards = np.append(self.rewards[path_slice], last_value)\n",
        "        values = np.append(self.values[path_slice], last_value)\n",
        "\n",
        "        gae = 0.0\n",
        "        adv = np.zeros_like(self.rewards[path_slice])\n",
        "\n",
        "        for t in reversed(range(len(rewards) - 1)):\n",
        "            delta = rewards[t] + gamma * values[t + 1] * (1 - self.dones[path_slice][t]) - values[t]\n",
        "            gae = delta + gamma * lam * (1 - self.dones[path_slice][t]) * gae\n",
        "            adv[t] = gae\n",
        "\n",
        "        self.advantages[path_slice] = adv\n",
        "        self.returns[path_slice] = adv + self.values[path_slice]\n",
        "        self.path_start_idx = self.ptr\n",
        "\n",
        "    def get(self):\n",
        "        assert self.ptr == self.size\n",
        "        self.ptr = 0\n",
        "        self.path_start_idx = 0\n",
        "\n",
        "        adv = self.advantages\n",
        "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "\n",
        "        return dict(\n",
        "            obs=torch.tensor(self.obs, dtype=torch.float32, device=self.device),\n",
        "            actions=torch.tensor(self.actions, dtype=torch.float32, device=self.device),\n",
        "            log_probs=torch.tensor(self.log_probs, dtype=torch.float32, device=self.device),\n",
        "            advantages=torch.tensor(adv, dtype=torch.float32, device=self.device),\n",
        "            returns=torch.tensor(self.returns, dtype=torch.float32, device=self.device),\n",
        "            values=torch.tensor(self.values, dtype=torch.float32, device=self.device),\n",
        "        )\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_id=\"BipedalWalker-v3\",\n",
        "        total_timesteps=2_000_000,\n",
        "        rollout_steps=4096,\n",
        "        gamma=0.99,\n",
        "        lam=0.95,\n",
        "        clip_eps=0.2,\n",
        "        learning_rate=2.5e-4,\n",
        "        train_epochs=10,\n",
        "        minibatch_size=512,\n",
        "        vf_coef=0.5,\n",
        "        ent_coef=0.00,\n",
        "        render_freq=25,\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        self.env_id = env_id\n",
        "        self.total_timesteps = total_timesteps\n",
        "        self.rollout_steps = rollout_steps\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.clip_eps = clip_eps\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_epochs = train_epochs\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.vf_coef = vf_coef\n",
        "        self.ent_coef = ent_coef\n",
        "        self.render_freq = render_freq\n",
        "        self.device = device\n",
        "\n",
        "        self.ep_returns = []\n",
        "        self.ep_timesteps = []\n",
        "\n",
        "        self.env = gym.make(env_id)\n",
        "        self.env = gym.wrappers.RecordEpisodeStatistics(self.env)\n",
        "        self.env = gym.wrappers.ClipAction(self.env)\n",
        "\n",
        "        self.env = gym.wrappers.NormalizeObservation(self.env)\n",
        "        self.env = gym.wrappers.TransformObservation(self.env, lambda obs: np.clip(obs, -10, 10), self.env.observation_space)\n",
        "\n",
        "        self.obs_dim = self.env.observation_space.shape[0]\n",
        "        self.act_dim = self.env.action_space.shape[0]\n",
        "\n",
        "        self.ac = ActorCritic(self.obs_dim, self.act_dim).to(device)\n",
        "        self.optimizer = optim.Adam(self.ac.parameters(), lr=self.learning_rate, eps=1e-5)\n",
        "\n",
        "        self.num_updates = total_timesteps // rollout_steps\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            self.optimizer, start_factor=1.0, end_factor=0.0, total_iters=self.num_updates\n",
        "        )\n",
        "\n",
        "        self.buffer = RolloutBuffer(self.rollout_steps, self.obs_dim, self.act_dim, self.device)\n",
        "\n",
        "    def visualize_agent(self, update_count):\n",
        "        print(f\"\\n--- Visualizing Agent at Update {update_count} ---\")\n",
        "        vis_env = gym.make(self.env_id, render_mode=\"rgb_array\")\n",
        "        vis_env = gym.wrappers.RecordVideo(\n",
        "            vis_env,\n",
        "            video_folder=\"videos\",\n",
        "            name_prefix=f\"update_{update_count}\",\n",
        "            disable_logger=True\n",
        "        )\n",
        "        vis_env = gym.wrappers.ClipAction(vis_env)\n",
        "        vis_norm = gym.wrappers.NormalizeObservation(vis_env)\n",
        "        try:\n",
        "            vis_norm.obs_rms = self.env.get_wrapper_attr('obs_rms')\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        vis_env = gym.wrappers.TransformObservation(vis_norm, lambda obs: np.clip(obs, -10, 10), vis_env.observation_space)\n",
        "\n",
        "        obs, _ = vis_env.reset()\n",
        "        ret = 0\n",
        "        while True:\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = self.ac.actor_mean(obs_tensor).squeeze(0).cpu().numpy()\n",
        "            obs, reward, terminated, truncated, _ = vis_env.step(action)\n",
        "            ret += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        vis_env.close()\n",
        "        print(f\"Visualization finished with return: {ret:.2f}\")\n",
        "        show_video(\"videos\")\n",
        "\n",
        "    def train(self):\n",
        "        obs, _ = self.env.reset()\n",
        "        timesteps_collected = 0\n",
        "        update_count = 0\n",
        "\n",
        "        while timesteps_collected < self.total_timesteps:\n",
        "            for _ in range(self.rollout_steps):\n",
        "                obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    action, log_prob, _, value = self.ac.get_action_and_value(obs_tensor)\n",
        "\n",
        "                action = action.cpu().numpy().squeeze(0)\n",
        "                log_prob = log_prob.item()\n",
        "                value = value.item()\n",
        "\n",
        "                next_obs, reward, terminated, truncated, infos = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                self.buffer.store(obs, action, log_prob, reward, done, value)\n",
        "                timesteps_collected += 1\n",
        "                obs = next_obs\n",
        "\n",
        "                if \"episode\" in infos:\n",
        "                    ret = infos['episode']['r']\n",
        "                    self.ep_returns.append(ret)\n",
        "                    self.ep_timesteps.append(timesteps_collected)\n",
        "                    print(f\"Update {update_count} | Steps: {timesteps_collected} | Return: {ret:.2f}\")\n",
        "\n",
        "                if done:\n",
        "                    if truncated:\n",
        "                        last_val_obs = torch.tensor(next_obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                        with torch.no_grad():\n",
        "                            last_value = self.ac.get_value(last_val_obs).item()\n",
        "                    else:\n",
        "                        last_value = 0\n",
        "                    self.buffer.finish_path(last_value=last_value, gamma=self.gamma, lam=self.lam)\n",
        "                    obs, _ = self.env.reset()\n",
        "\n",
        "                if timesteps_collected >= self.total_timesteps:\n",
        "                    break\n",
        "\n",
        "            if not done:\n",
        "                obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    last_value = self.ac.get_value(obs_tensor).item()\n",
        "                self.buffer.finish_path(last_value=last_value, gamma=self.gamma, lam=self.lam)\n",
        "\n",
        "            if self.buffer.ptr == self.buffer.size:\n",
        "                data = self.buffer.get()\n",
        "                self._update(data)\n",
        "                self.lr_scheduler.step()\n",
        "                update_count += 1\n",
        "\n",
        "                if update_count % 50 == 0:\n",
        "                    self.save(os.path.join(DRIVE_FOLDER, \"ppo_bipedal_checkpoint.pt\"))\n",
        "                    print(f\"Checkpoint saved to drive at update {update_count}\")\n",
        "\n",
        "            if update_count > 0 and update_count % self.render_freq == 0:\n",
        "                self.visualize_agent(update_count)\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def _update(self, data):\n",
        "        obs = data[\"obs\"]\n",
        "        actions = data[\"actions\"]\n",
        "        old_log_probs = data[\"log_probs\"]\n",
        "        advantages = data[\"advantages\"]\n",
        "        returns = data[\"returns\"]\n",
        "\n",
        "        batch_size = len(obs)\n",
        "        inds = np.arange(batch_size)\n",
        "\n",
        "        for _ in range(self.train_epochs):\n",
        "            np.random.shuffle(inds)\n",
        "            for start in range(0, batch_size, self.minibatch_size):\n",
        "                end = start + self.minibatch_size\n",
        "                mb_inds = inds[start:end]\n",
        "                mb_obs = obs[mb_inds]\n",
        "                mb_actions = actions[mb_inds]\n",
        "                mb_old_log_probs = old_log_probs[mb_inds]\n",
        "                mb_adv = advantages[mb_inds]\n",
        "                mb_returns = returns[mb_inds]\n",
        "\n",
        "                _, new_log_probs, entropy, values = self.ac.get_action_and_value(mb_obs, mb_actions)\n",
        "                ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
        "                surr1 = ratio * mb_adv\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * mb_adv\n",
        "\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                critic_loss = 0.5 * ((values - mb_returns) ** 2).mean()\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = actor_loss + self.vf_coef * critic_loss - self.ent_coef * entropy_loss\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.ac.parameters(), max_norm=0.5)\n",
        "                self.optimizer.step()\n",
        "\n",
        "    def plot_results(self, window_size=50):\n",
        "        if not self.ep_returns:\n",
        "            print(\"Can not plot, no rewards\")\n",
        "            return\n",
        "\n",
        "        # Calculate Running Average\n",
        "        returns = np.array(self.ep_returns)\n",
        "        running_avg = np.convolve(returns, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.ep_timesteps, returns, alpha=0.3, color='blue', label='Episode Return')\n",
        "\n",
        "        avg_timesteps = self.ep_timesteps[window_size-1:]\n",
        "        plt.plot(avg_timesteps, running_avg, color='red', linewidth=2, label=f'Running Avg (last {window_size} eps)')\n",
        "\n",
        "        plt.title(f\"PPO Training Progress: {self.env_id}\")\n",
        "        plt.xlabel(\"Total Timesteps\")\n",
        "        plt.ylabel(\"Return\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        # Save plot to drive\n",
        "        plot_path = os.path.join(DRIVE_FOLDER, \"training_plot.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        plt.show()\n",
        "        print(f\"Plot saved to: {plot_path}\")\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.ac.state_dict(), path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    agent = PPOAgent(\n",
        "        env_id=\"BipedalWalker-v3\",\n",
        "        total_timesteps=2_000_000,\n",
        "        rollout_steps=4096,\n",
        "        render_freq=25,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    agent.train()\n",
        "\n",
        "    # Final plot and save\n",
        "    agent.plot_results(window_size=50)\n",
        "\n",
        "    final_model_path = os.path.join(DRIVE_FOLDER, \"ppo_bipedal_final.pt\")\n",
        "    final_stats_path = os.path.join(DRIVE_FOLDER, \"obs_stats.pkl\")\n",
        "    agent.save(final_model_path)\n",
        "\n",
        "    with open(final_stats_path, \"wb\") as f:\n",
        "        pickle.dump(agent.env.get_wrapper_attr('obs_rms'), f)\n",
        "\n",
        "    print(f\"Training Complete. Files saved to Drive:\\n1. {final_model_path}\\n2. {final_stats_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ajzPBDqJvQ2"
      },
      "outputs": [],
      "source": [
        "import os, glob, io, base64, pickle, random\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/BipedalWalker_Project\"\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "\n",
        "BASE_NORMAL_MODEL = os.path.join(DRIVE_FOLDER, \"ppo_bipedal_final.pt\")\n",
        "BASE_NORMAL_STATS = os.path.join(DRIVE_FOLDER, \"obs_stats.pkl\")\n",
        "\n",
        "RUN_TAG = \"v11\"\n",
        "CKPT_PATH  = os.path.join(DRIVE_FOLDER, f\"ppo_hardcore_{RUN_TAG}_checkpoint.pt\")\n",
        "BEST_PATH  = os.path.join(DRIVE_FOLDER, f\"ppo_hardcore_{RUN_TAG}_best.pt\")\n",
        "FINAL_PATH = os.path.join(DRIVE_FOLDER, f\"ppo_hardcore_{RUN_TAG}_final.pt\")\n",
        "PLOT_PATH  = os.path.join(DRIVE_FOLDER, f\"training_plot_{RUN_TAG}.png\")\n",
        "\n",
        "print(\"Drive folder:\", DRIVE_FOLDER)\n",
        "print(\"Checkpoint:\", CKPT_PATH)\n",
        "print(\"Best:\", BEST_PATH)\n",
        "print(\"Final:\", FINAL_PATH)\n",
        "print(\"Plot:\", PLOT_PATH)\n",
        "\n",
        "\n",
        "def show_video(folder=\"videos\"):\n",
        "    mp4list = glob.glob(f\"{folder}/*.mp4\")\n",
        "    if len(mp4list) == 0:\n",
        "        print(\"No video found.\")\n",
        "        return\n",
        "    mp4 = max(mp4list, key=os.path.getctime)\n",
        "    video = io.open(mp4, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    display(HTML(data=f\"\"\"\n",
        "    <video autoplay loop controls style=\"height: 420px;\">\n",
        "      <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "\n",
        "def make_one_env_video(seed=None, video_folder=\"videos\", name_prefix=\"eval\"):\n",
        "    env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode=\"rgb_array\")\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = gym.wrappers.NormalizeObservation(env)\n",
        "    env = gym.wrappers.TransformObservation(env, lambda o: np.clip(o, -10, 10), env.observation_space)\n",
        "\n",
        "    env = gym.wrappers.RecordVideo(\n",
        "        env,\n",
        "        video_folder=video_folder,\n",
        "        episode_trigger=lambda ep: True,\n",
        "        name_prefix=name_prefix,\n",
        "        disable_logger=True\n",
        "    )\n",
        "\n",
        "    if seed is not None:\n",
        "        env.reset(seed=int(seed))\n",
        "    return env\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    nn.init.orthogonal_(layer.weight, std)\n",
        "    nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 1), std=1.0),\n",
        "        )\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, act_dim), std=0.01),\n",
        "        )\n",
        "        self.actor_logstd = nn.Parameter(torch.ones(1, act_dim) * -0.7)\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x).squeeze(1)\n",
        "\n",
        "    def _dist(self, x):\n",
        "        mean = self.actor_mean(x)\n",
        "        logstd = self.actor_logstd.expand_as(mean)\n",
        "        std = torch.exp(logstd)\n",
        "        return Normal(mean, std), mean, logstd\n",
        "\n",
        "    @staticmethod\n",
        "    def _squash_action(u):\n",
        "        return torch.tanh(u)\n",
        "\n",
        "    @staticmethod\n",
        "    def _squash_logprob(dist, u, eps=1e-6):\n",
        "        logp_u = dist.log_prob(u).sum(dim=1)\n",
        "        a = torch.tanh(u)\n",
        "        corr = torch.log(1.0 - a.pow(2) + eps).sum(dim=1)\n",
        "        return logp_u - corr\n",
        "\n",
        "    def get_action_and_value(self, x, action=None, deterministic=False):\n",
        "        dist, mean, _ = self._dist(x)\n",
        "\n",
        "        if action is None:\n",
        "            if deterministic:\n",
        "                u = mean\n",
        "            else:\n",
        "                u = dist.rsample()\n",
        "            a = self._squash_action(u)\n",
        "            logp = self._squash_logprob(dist, u)\n",
        "        else:\n",
        "            a = action\n",
        "            a_clamped = torch.clamp(a, -0.999, 0.999)\n",
        "            u = 0.5 * (torch.log1p(a_clamped) - torch.log1p(-a_clamped))  # atanh\n",
        "            logp = self._squash_logprob(dist, u)\n",
        "\n",
        "        ent = dist.entropy().sum(dim=1)\n",
        "        val = self.get_value(x)\n",
        "        mean_a = torch.tanh(mean)\n",
        "        return a, logp, ent, val, mean_a\n",
        "\n",
        "\n",
        "def make_one_env(seed=None):\n",
        "    env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = gym.wrappers.NormalizeObservation(env)\n",
        "    env = gym.wrappers.TransformObservation(env, lambda o: np.clip(o, -10, 10), env.observation_space)\n",
        "    if seed is not None:\n",
        "        env.reset(seed=int(seed))\n",
        "    return env\n",
        "\n",
        "\n",
        "def make_vec_env(n, seed_list=None):\n",
        "    def thunk(i):\n",
        "        def _f():\n",
        "            s = None if seed_list is None else seed_list[i]\n",
        "            return make_one_env(seed=s)\n",
        "        return _f\n",
        "    return gym.vector.SyncVectorEnv([thunk(i) for i in range(n)])\n",
        "\n",
        "\n",
        "def find_normalize_obs_wrapper(env):\n",
        "    env_ptr = env\n",
        "    while hasattr(env_ptr, \"env\"):\n",
        "        if isinstance(env_ptr, gym.wrappers.NormalizeObservation):\n",
        "            return env_ptr\n",
        "        env_ptr = env_ptr.env\n",
        "    return None\n",
        "\n",
        "\n",
        "class SeedPool:\n",
        "    def __init__(self, maxlen=4000):\n",
        "        self.maxlen = maxlen\n",
        "        self.seeds = deque(maxlen=maxlen)\n",
        "\n",
        "    def add(self, seed):\n",
        "        if seed is None:\n",
        "            return\n",
        "        self.seeds.append(int(seed))\n",
        "\n",
        "    def sample(self, k):\n",
        "        if len(self.seeds) == 0:\n",
        "            return None\n",
        "        k = min(k, len(self.seeds))\n",
        "        return random.sample(list(self.seeds), k)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\"maxlen\": self.maxlen, \"seeds\": list(self.seeds)}\n",
        "\n",
        "    def load_state_dict(self, d):\n",
        "        self.maxlen = int(d.get(\"maxlen\", self.maxlen))\n",
        "        self.seeds = deque(d.get(\"seeds\", []), maxlen=self.maxlen)\n",
        "\n",
        "\n",
        "class PPOHardcoreVectorV11:\n",
        "    def __init__(\n",
        "        self,\n",
        "        device=\"cuda\",\n",
        "        num_envs=16,\n",
        "        rollout_steps=2048,          # per env\n",
        "        total_env_steps=40_000_000,  # across envs\n",
        "        gamma=0.99,\n",
        "        lam=0.95,\n",
        "        clip_eps=0.15,\n",
        "        lr=1.5e-4,\n",
        "        train_epochs=10,\n",
        "        minibatch_size=4096,\n",
        "        vf_coef=0.5,\n",
        "        ent_coef_start=0.02,\n",
        "        ent_coef_end=0.0,\n",
        "        max_grad_norm=0.5,\n",
        "        target_kl=0.02,\n",
        "\n",
        "        logstd_min=-5.0,\n",
        "        logstd_max=-0.5,\n",
        "\n",
        "        jerk_start_p75=240.0,\n",
        "        jerk_coef_max=0.0010,\n",
        "        jerk_ramp_updates=200,\n",
        "\n",
        "        eval_every_updates=10,\n",
        "        eval_episodes=16,\n",
        "\n",
        "        checkpoint_every_updates=10,\n",
        "\n",
        "        reseed_p_random=0.60,\n",
        "        reseed_p_hard=0.20,\n",
        "        reseed_p_success=0.20,\n",
        "\n",
        "        hard_pool_bottom_frac=0.20,\n",
        "        success_pool_threshold=300.0,\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.num_envs = num_envs\n",
        "        self.rollout_steps = rollout_steps\n",
        "        self.total_env_steps = total_env_steps\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.clip_eps = clip_eps\n",
        "        self.lr = lr\n",
        "        self.train_epochs = train_epochs\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.vf_coef = vf_coef\n",
        "        self.ent_coef_start = ent_coef_start\n",
        "        self.ent_coef_end = ent_coef_end\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.target_kl = target_kl\n",
        "\n",
        "        self.logstd_min = logstd_min\n",
        "        self.logstd_max = logstd_max\n",
        "\n",
        "        self.jerk_start_p75 = jerk_start_p75\n",
        "        self.jerk_coef_max = jerk_coef_max\n",
        "        self.jerk_ramp_updates = max(1, int(jerk_ramp_updates))\n",
        "\n",
        "        self.eval_every_updates = eval_every_updates\n",
        "        self.eval_episodes = eval_episodes\n",
        "        self.checkpoint_every_updates = checkpoint_every_updates\n",
        "\n",
        "        s = reseed_p_random + reseed_p_hard + reseed_p_success\n",
        "        self.reseed_p_random  = reseed_p_random / s\n",
        "        self.reseed_p_hard    = reseed_p_hard / s\n",
        "        self.reseed_p_success = reseed_p_success / s\n",
        "\n",
        "        self.hard_pool_bottom_frac = hard_pool_bottom_frac\n",
        "        self.success_pool_threshold = success_pool_threshold\n",
        "\n",
        "        self.hard_pool = SeedPool(maxlen=4000)\n",
        "        self.success_pool = SeedPool(maxlen=2000)\n",
        "\n",
        "        self.env_seeds = [random.randint(0, 2_000_000_000) for _ in range(num_envs)]\n",
        "        self.env = make_vec_env(num_envs, seed_list=self.env_seeds)\n",
        "\n",
        "        self.obs_dim = self.env.single_observation_space.shape[0]\n",
        "        self.act_dim = self.env.single_action_space.shape[0]\n",
        "\n",
        "        self.ac = ActorCritic(self.obs_dim, self.act_dim).to(self.device)\n",
        "        self.opt = optim.Adam(self.ac.parameters(), lr=self.lr, eps=1e-5)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.update = 0\n",
        "\n",
        "        self.best_eval_p75 = -1e9\n",
        "\n",
        "        self.jerk_enabled = False\n",
        "        self.jerk_enable_update = None\n",
        "        self.jerk_enable_step = None\n",
        "        self.jerk_coef_current = 0.0\n",
        "\n",
        "        T, N = rollout_steps, num_envs\n",
        "        self.buf_obs  = np.zeros((T, N, self.obs_dim), dtype=np.float32)\n",
        "        self.buf_act  = np.zeros((T, N, self.act_dim), dtype=np.float32)\n",
        "        self.buf_logp = np.zeros((T, N), dtype=np.float32)\n",
        "        self.buf_rew  = np.zeros((T, N), dtype=np.float32)\n",
        "        self.buf_done = np.zeros((T, N), dtype=np.float32)\n",
        "        self.buf_val  = np.zeros((T, N), dtype=np.float32)\n",
        "\n",
        "        self.buf_vold = np.zeros((T, N), dtype=np.float32)\n",
        "\n",
        "        self.prev_mean = np.zeros((N, self.act_dim), dtype=np.float32)\n",
        "\n",
        "        self.ep_returns = []\n",
        "        self.ep_timesteps = []\n",
        "        self.eval_history = []\n",
        "\n",
        "        if os.path.exists(CKPT_PATH):\n",
        "            print(f\"Resuming from checkpoint: {CKPT_PATH}\")\n",
        "            self.load(CKPT_PATH)\n",
        "        else:\n",
        "            if os.path.exists(BASE_NORMAL_MODEL):\n",
        "                w = torch.load(BASE_NORMAL_MODEL, map_location=self.device, weights_only=False)\n",
        "                self.ac.load_state_dict(w)\n",
        "                with torch.no_grad():\n",
        "                    self.ac.actor_logstd.fill_(-0.7)\n",
        "\n",
        "            if os.path.exists(BASE_NORMAL_STATS):\n",
        "                with open(BASE_NORMAL_STATS, \"rb\") as f:\n",
        "                    base_rms = pickle.load(f)\n",
        "                for e in self.env.envs:\n",
        "                    wno = find_normalize_obs_wrapper(e)\n",
        "                    if wno is not None:\n",
        "                        wno.obs_rms = base_rms\n",
        "\n",
        "    def clamp_logstd(self):\n",
        "        with torch.no_grad():\n",
        "            self.ac.actor_logstd.clamp_(self.logstd_min, self.logstd_max)\n",
        "\n",
        "    def current_ent_coef(self):\n",
        "        frac = max(0.0, 1.0 - (self.global_step / self.total_env_steps))\n",
        "        return self.ent_coef_end + (self.ent_coef_start - self.ent_coef_end) * frac\n",
        "\n",
        "    def current_jerk_coef(self):\n",
        "        if not self.jerk_enabled:\n",
        "            return 0.0\n",
        "        k = self.update - int(self.jerk_enable_update)\n",
        "        k = max(0, min(k, self.jerk_ramp_updates))\n",
        "        return self.jerk_coef_max * (k / self.jerk_ramp_updates)\n",
        "\n",
        "    def reseed_envs_if_needed(self, done_mask):\n",
        "        done_idxs = np.where(done_mask)[0]\n",
        "        if len(done_idxs) == 0:\n",
        "            return\n",
        "\n",
        "        hard = self.hard_pool.sample(len(done_idxs))\n",
        "        succ = self.success_pool.sample(len(done_idxs))\n",
        "\n",
        "        for j, env_i in enumerate(done_idxs):\n",
        "            r = random.random()\n",
        "            if (hard is not None) and (r < self.reseed_p_hard):\n",
        "                seed = hard[j % len(hard)]\n",
        "            elif (succ is not None) and (r < self.reseed_p_hard + self.reseed_p_success):\n",
        "                seed = succ[j % len(succ)]\n",
        "            else:\n",
        "                seed = random.randint(0, 2_000_000_000)\n",
        "\n",
        "            self.env_seeds[env_i] = seed\n",
        "            self.env.envs[env_i].reset(seed=int(seed))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, episodes=16, deterministic=True):\n",
        "        env = make_one_env()\n",
        "\n",
        "        train_wno = find_normalize_obs_wrapper(self.env.envs[0])\n",
        "        eval_wno  = find_normalize_obs_wrapper(env)\n",
        "        if train_wno is not None and eval_wno is not None:\n",
        "            eval_wno.obs_rms = train_wno.obs_rms\n",
        "\n",
        "        seed_ret = []\n",
        "        for _ in range(episodes):\n",
        "            seed = random.randint(0, 2_000_000_000)\n",
        "            obs, _ = env.reset(seed=int(seed))\n",
        "            ep_ret = 0.0\n",
        "            done = False\n",
        "            while not done:\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                a, _, _, _, _ = self.ac.get_action_and_value(obs_t, action=None, deterministic=deterministic)\n",
        "                obs, r, term, trunc, _ = env.step(a.squeeze(0).cpu().numpy())\n",
        "                ep_ret += float(r)\n",
        "                done = term or trunc\n",
        "            seed_ret.append((seed, ep_ret))\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        rets = np.array([x[1] for x in seed_ret], dtype=np.float32)\n",
        "        mean = float(np.mean(rets))\n",
        "        med  = float(np.median(rets))\n",
        "        p75  = float(np.percentile(rets, 75))\n",
        "        best = float(np.max(rets))\n",
        "        worst= float(np.min(rets))\n",
        "        return mean, med, p75, best, worst, seed_ret\n",
        "\n",
        "    def save(self, path):\n",
        "        wno = find_normalize_obs_wrapper(self.env.envs[0])\n",
        "        ckpt = {\n",
        "            \"run_tag\": RUN_TAG,\n",
        "            \"model\": self.ac.state_dict(),\n",
        "            \"opt\": self.opt.state_dict(),\n",
        "            \"global_step\": self.global_step,\n",
        "            \"update\": self.update,\n",
        "            \"best_eval_p75\": self.best_eval_p75,\n",
        "\n",
        "            \"jerk_enabled\": self.jerk_enabled,\n",
        "            \"jerk_enable_update\": self.jerk_enable_update,\n",
        "            \"jerk_enable_step\": self.jerk_enable_step,\n",
        "\n",
        "            \"obs_rms\": (wno.obs_rms if wno is not None else None),\n",
        "            \"hard_pool\": self.hard_pool.state_dict(),\n",
        "            \"success_pool\": self.success_pool.state_dict(),\n",
        "            \"env_seeds\": self.env_seeds,\n",
        "\n",
        "            \"ep_returns\": self.ep_returns,\n",
        "            \"ep_timesteps\": self.ep_timesteps,\n",
        "            \"eval_history\": self.eval_history,\n",
        "        }\n",
        "        torch.save(ckpt, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        ckpt = torch.load(path, map_location=self.device, weights_only=False)\n",
        "\n",
        "        self.ac.load_state_dict(ckpt[\"model\"])\n",
        "        self.opt.load_state_dict(ckpt[\"opt\"])\n",
        "        self.global_step = int(ckpt.get(\"global_step\", 0))\n",
        "        self.update = int(ckpt.get(\"update\", 0))\n",
        "        self.best_eval_p75 = float(ckpt.get(\"best_eval_p75\", -1e9))\n",
        "\n",
        "        self.jerk_enabled = bool(ckpt.get(\"jerk_enabled\", False))\n",
        "        self.jerk_enable_update = ckpt.get(\"jerk_enable_update\", None)\n",
        "        self.jerk_enable_step = ckpt.get(\"jerk_enable_step\", None)\n",
        "\n",
        "        hp = ckpt.get(\"hard_pool\", None)\n",
        "        if hp is not None:\n",
        "            self.hard_pool.load_state_dict(hp)\n",
        "\n",
        "        sp = ckpt.get(\"success_pool\", None)\n",
        "        if sp is not None:\n",
        "            self.success_pool.load_state_dict(sp)\n",
        "\n",
        "        env_seeds = ckpt.get(\"env_seeds\", None)\n",
        "        if env_seeds is not None and len(env_seeds) == self.num_envs:\n",
        "            self.env_seeds = list(env_seeds)\n",
        "\n",
        "        self.ep_returns = list(ckpt.get(\"ep_returns\", []))\n",
        "        self.ep_timesteps = list(ckpt.get(\"ep_timesteps\", []))\n",
        "        self.eval_history = list(ckpt.get(\"eval_history\", []))\n",
        "\n",
        "        try:\n",
        "            self.env.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.env = make_vec_env(self.num_envs, seed_list=self.env_seeds)\n",
        "\n",
        "        rms = ckpt.get(\"obs_rms\", None)\n",
        "        if rms is not None:\n",
        "            for e in self.env.envs:\n",
        "                wno = find_normalize_obs_wrapper(e)\n",
        "                if wno is not None:\n",
        "                    wno.obs_rms = rms\n",
        "\n",
        "        print(\n",
        "            f\"Loaded ckpt: upd={self.update} step={self.global_step} \"\n",
        "            f\"jerk={'on' if self.jerk_enabled else 'off'} \"\n",
        "            f\"hard={len(self.hard_pool.seeds)} success={len(self.success_pool.seeds)} \"\n",
        "            f\"best_p75={self.best_eval_p75:.2f}\"\n",
        "        )\n",
        "\n",
        "    def plot_results(self, window_size=50):\n",
        "        if not self.ep_returns:\n",
        "            print(\"No rewards to plot\")\n",
        "            return\n",
        "\n",
        "        returns = np.array(self.ep_returns, dtype=np.float32)\n",
        "        timesteps = np.array(self.ep_timesteps, dtype=np.int64)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        plt.plot(\n",
        "            timesteps,\n",
        "            returns,\n",
        "            alpha=0.3,\n",
        "            color=\"blue\",\n",
        "            linewidth=1.0,\n",
        "            label=\"Raw Episode Return\"\n",
        "        )\n",
        "\n",
        "        if len(returns) >= window_size:\n",
        "            running_avg = np.convolve(returns, np.ones(window_size) / window_size, mode=\"valid\")\n",
        "            avg_timesteps = timesteps[window_size - 1:]\n",
        "            plt.plot(\n",
        "                avg_timesteps,\n",
        "                running_avg,\n",
        "                color=\"red\",\n",
        "                linewidth=2.0,\n",
        "                label=f\"Running Avg ({window_size})\"\n",
        "            )\n",
        "\n",
        "        if self.jerk_enable_step is not None:\n",
        "            plt.axvline(\n",
        "                x=self.jerk_enable_step,\n",
        "                color=\"green\",\n",
        "                linestyle=\"--\",\n",
        "                linewidth=2.0,\n",
        "                label=\"Jerk penalty on\"\n",
        "            )\n",
        "\n",
        "        plt.title(\"BipedalWalker Hardcore: Full Training Progress\")\n",
        "        plt.xlabel(\"Environment Timesteps\")\n",
        "        plt.ylabel(\"Return\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
        "\n",
        "        plt.savefig(PLOT_PATH, dpi=160, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        print(f\"Plot saved to: {PLOT_PATH}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def record_eval_video(self, deterministic=True, max_steps=2000):\n",
        "        video_folder = os.path.join(DRIVE_FOLDER, f\"videos_{RUN_TAG}\")\n",
        "        os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "        name_prefix = f\"upd{self.update}_step{self.global_step}\"\n",
        "        env = make_one_env_video(\n",
        "            seed=random.randint(0, 2_000_000_000),\n",
        "            video_folder=video_folder,\n",
        "            name_prefix=name_prefix,\n",
        "        )\n",
        "\n",
        "        train_wno = find_normalize_obs_wrapper(self.env.envs[0])\n",
        "        eval_wno  = find_normalize_obs_wrapper(env)\n",
        "        if train_wno is not None and eval_wno is not None:\n",
        "            eval_wno.obs_rms = train_wno.obs_rms\n",
        "\n",
        "        obs, _ = env.reset()\n",
        "        ep_ret = 0.0\n",
        "        for _ in range(max_steps):\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            a, _, _, _, _ = self.ac.get_action_and_value(obs_t, action=None, deterministic=deterministic)\n",
        "            obs, r, term, trunc, _ = env.step(a.squeeze(0).cpu().numpy())\n",
        "            ep_ret += float(r)\n",
        "            if term or trunc:\n",
        "                break\n",
        "\n",
        "        env.close()\n",
        "        print(f\"Evaluation video | ret={ep_ret:.2f} | folder={video_folder}\")\n",
        "        show_video(video_folder)\n",
        "\n",
        "    def train(self):\n",
        "        obs, _ = self.env.reset()\n",
        "        self.prev_mean[:] = 0.0\n",
        "\n",
        "        batch_size = self.num_envs * self.rollout_steps\n",
        "\n",
        "        while self.global_step < self.total_env_steps:\n",
        "            self.clamp_logstd()\n",
        "            ent_coef = self.current_ent_coef()\n",
        "            self.jerk_coef_current = self.current_jerk_coef()\n",
        "\n",
        "            for t in range(self.rollout_steps):\n",
        "                self.buf_obs[t] = obs\n",
        "\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
        "                with torch.no_grad():\n",
        "                    action, logp, ent, val, mean_a = self.ac.get_action_and_value(obs_t)\n",
        "\n",
        "                act_np = action.cpu().numpy().astype(np.float32)\n",
        "                mean_np = mean_a.cpu().numpy().astype(np.float32)\n",
        "\n",
        "                next_obs, raw_r, term, trunc, infos = self.env.step(act_np)\n",
        "                done = np.logical_or(term, trunc)\n",
        "\n",
        "                shaped = raw_r.astype(np.float32)\n",
        "                if self.jerk_coef_current > 0.0:\n",
        "                    da2 = np.mean((mean_np - self.prev_mean) ** 2, axis=1)\n",
        "                    shaped = shaped - (self.jerk_coef_current * da2).astype(np.float32)\n",
        "\n",
        "                self.prev_mean = mean_np\n",
        "\n",
        "                self.buf_act[t]  = act_np\n",
        "                self.buf_logp[t] = logp.cpu().numpy().astype(np.float32)\n",
        "                self.buf_val[t]  = val.cpu().numpy().astype(np.float32)\n",
        "                self.buf_vold[t] = self.buf_val[t]\n",
        "                self.buf_rew[t]  = shaped\n",
        "                self.buf_done[t] = done.astype(np.float32)\n",
        "\n",
        "                obs = next_obs\n",
        "                self.global_step += self.num_envs\n",
        "\n",
        "                if isinstance(infos, dict) and \"episode\" in infos:\n",
        "                    ep_r = infos[\"episode\"][\"r\"]\n",
        "                    ep_l = infos[\"episode\"][\"l\"]\n",
        "                    for i in range(self.num_envs):\n",
        "                        if done[i]:\n",
        "                            r_i = float(ep_r[i])\n",
        "                            l_i = int(ep_l[i])\n",
        "                            print(\n",
        "                                f\"step={self.global_step:9d} | upd={self.update:6d} | \"\n",
        "                                f\"env={i:2d} | ret={r_i:7.2f} | len={l_i:4d} | \"\n",
        "                                f\"jerk={'on' if self.jerk_coef_current > 0.0 else 'off'} coef={self.jerk_coef_current:.5f}\"\n",
        "                            )\n",
        "                            self.ep_returns.append(r_i)\n",
        "                            self.ep_timesteps.append(int(self.global_step))\n",
        "\n",
        "                self.reseed_envs_if_needed(done)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
        "                next_val = self.ac.get_value(obs_t).cpu().numpy().astype(np.float32)\n",
        "\n",
        "            adv = np.zeros((self.rollout_steps, self.num_envs), dtype=np.float32)\n",
        "            lastgaelam = np.zeros((self.num_envs,), dtype=np.float32)\n",
        "            for t in reversed(range(self.rollout_steps)):\n",
        "                nonterminal = 1.0 - self.buf_done[t]\n",
        "                nextv = next_val if t == self.rollout_steps - 1 else self.buf_val[t + 1]\n",
        "                delta = self.buf_rew[t] + self.gamma * nextv * nonterminal - self.buf_val[t]\n",
        "                lastgaelam = delta + self.gamma * self.lam * nonterminal * lastgaelam\n",
        "                adv[t] = lastgaelam\n",
        "            ret = adv + self.buf_val\n",
        "\n",
        "            b_obs  = torch.tensor(self.buf_obs.reshape(batch_size, self.obs_dim), device=self.device)\n",
        "            b_act  = torch.tensor(self.buf_act.reshape(batch_size, self.act_dim), device=self.device)\n",
        "            b_log  = torch.tensor(self.buf_logp.reshape(batch_size), device=self.device)\n",
        "            b_adv  = torch.tensor(adv.reshape(batch_size), device=self.device)\n",
        "            b_ret  = torch.tensor(ret.reshape(batch_size), device=self.device)\n",
        "            b_vold = torch.tensor(self.buf_vold.reshape(batch_size), device=self.device)\n",
        "\n",
        "            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
        "\n",
        "            inds = np.arange(batch_size)\n",
        "            approx_kl = 0.0\n",
        "\n",
        "            for g in self.opt.param_groups:\n",
        "                g[\"lr\"] = self.lr\n",
        "\n",
        "            for _ in range(self.train_epochs):\n",
        "                np.random.shuffle(inds)\n",
        "                for start in range(0, batch_size, self.minibatch_size):\n",
        "                    mb = inds[start : start + self.minibatch_size]\n",
        "\n",
        "                    _, new_logp, ent, new_val, _ = self.ac.get_action_and_value(b_obs[mb], b_act[mb])\n",
        "\n",
        "                    log_ratio = new_logp - b_log[mb]\n",
        "                    ratio = torch.exp(log_ratio)\n",
        "\n",
        "                    surr1 = ratio * b_adv[mb]\n",
        "                    surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * b_adv[mb]\n",
        "                    actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                    v_pred = new_val\n",
        "                    v_old = b_vold[mb]\n",
        "                    v_clipped = v_old + torch.clamp(v_pred - v_old, -0.2, 0.2)\n",
        "\n",
        "                    v_loss_unclipped = (v_pred - b_ret[mb]).pow(2)\n",
        "                    v_loss_clipped   = (v_clipped - b_ret[mb]).pow(2)\n",
        "                    v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
        "\n",
        "                    ent_loss = ent.mean()\n",
        "\n",
        "                    loss = actor_loss + self.vf_coef * v_loss - ent_coef * ent_loss\n",
        "\n",
        "                    self.opt.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(self.ac.parameters(), self.max_grad_norm)\n",
        "                    self.opt.step()\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        approx_kl = float((ratio - 1.0 - log_ratio).mean().item())\n",
        "\n",
        "                    if approx_kl > 1.5 * self.target_kl:\n",
        "                        break\n",
        "                if approx_kl > 1.5 * self.target_kl:\n",
        "                    break\n",
        "\n",
        "            for g in self.opt.param_groups:\n",
        "                if approx_kl < 0.5 * self.target_kl:\n",
        "                    g[\"lr\"] = min(g[\"lr\"] * 1.10, 3e-4)\n",
        "                elif approx_kl > 1.5 * self.target_kl:\n",
        "                    g[\"lr\"] = max(g[\"lr\"] * 0.70, 1e-5)\n",
        "\n",
        "            self.update += 1\n",
        "\n",
        "            do_eval = (self.update % self.eval_every_updates == 0)\n",
        "            if do_eval:\n",
        "                mean_raw, med_raw, p75_raw, best_raw, worst_raw, seed_ret = self.evaluate(\n",
        "                    episodes=self.eval_episodes, deterministic=True\n",
        "                )\n",
        "\n",
        "                print(\n",
        "                    f\"Evaluation | upd {self.update:5d} | step {self.global_step:9d} | \"\n",
        "                    f\"mean {mean_raw:7.2f} median {med_raw:7.2f} p75 {p75_raw:7.2f} \"\n",
        "                    f\"best {best_raw:7.2f} worst {worst_raw:7.2f} | \"\n",
        "                    f\"approx_kl {approx_kl:.4f} | ent {ent_coef:.4f} | \"\n",
        "                    f\"jerk_coef {self.jerk_coef_current:.5f} | hard {len(self.hard_pool.seeds)} success {len(self.success_pool.seeds)}\"\n",
        "                )\n",
        "\n",
        "                seed_ret_sorted = sorted(seed_ret, key=lambda x: x[1])  # ascending by return\n",
        "                k = max(1, int(len(seed_ret_sorted) * self.hard_pool_bottom_frac))\n",
        "                hard_seeds = [s for (s, r) in seed_ret_sorted[:k]]\n",
        "                for s in hard_seeds:\n",
        "                    self.hard_pool.add(s)\n",
        "\n",
        "                for (s, r) in seed_ret_sorted:\n",
        "                    if r >= self.success_pool_threshold:\n",
        "                        self.success_pool.add(s)\n",
        "\n",
        "                if (not self.jerk_enabled) and (p75_raw >= self.jerk_start_p75):\n",
        "                    self.jerk_enabled = True\n",
        "                    self.jerk_enable_update = int(self.update)\n",
        "                    self.jerk_enable_step = int(self.global_step)\n",
        "                    print(\n",
        "                        f\"Jerk penalty started (eval p75 >= {self.jerk_start_p75}). \"\n",
        "                        f\"Increasing to {self.jerk_coef_max} over {self.jerk_ramp_updates} updates.\"\n",
        "                    )\n",
        "\n",
        "                if p75_raw > self.best_eval_p75:\n",
        "                    self.best_eval_p75 = p75_raw\n",
        "                    torch.save(self.ac.state_dict(), BEST_PATH)\n",
        "                    print(f\"New best saved -> {BEST_PATH}\")\n",
        "\n",
        "                self.eval_history.append({\n",
        "                    \"update\": int(self.update),\n",
        "                    \"step\": int(self.global_step),\n",
        "                    \"mean\": float(mean_raw),\n",
        "                    \"median\": float(med_raw),\n",
        "                    \"p75\": float(p75_raw),\n",
        "                    \"best\": float(best_raw),\n",
        "                    \"worst\": float(worst_raw),\n",
        "                    \"approx_kl\": float(approx_kl),\n",
        "                    \"ent_coef\": float(ent_coef),\n",
        "                    \"jerk_coef\": float(self.jerk_coef_current),\n",
        "                    \"hard_pool\": int(len(self.hard_pool.seeds)),\n",
        "                    \"success_pool\": int(len(self.success_pool.seeds)),\n",
        "                })\n",
        "\n",
        "                self.save(CKPT_PATH)\n",
        "                self.plot_results(window_size=50)\n",
        "                self.record_eval_video(deterministic=True)\n",
        "\n",
        "            if (not do_eval) and (self.update % self.checkpoint_every_updates == 0):\n",
        "                self.save(CKPT_PATH)\n",
        "                print(f\"Checkpoint saved -> {CKPT_PATH}\")\n",
        "\n",
        "            if self.best_eval_p75 >= 300.0:\n",
        "                break\n",
        "\n",
        "        torch.save(self.ac.state_dict(), FINAL_PATH)\n",
        "        self.save(CKPT_PATH)\n",
        "        print(\"Final model saved:\", FINAL_PATH)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    trainer = PPOHardcoreVectorV11(\n",
        "        device=device,\n",
        "        num_envs=16,\n",
        "        rollout_steps=2048,\n",
        "        total_env_steps=40_000_000,\n",
        "\n",
        "        lr=1.5e-4,\n",
        "        clip_eps=0.15,\n",
        "        target_kl=0.02,\n",
        "\n",
        "        ent_coef_start=0.02,\n",
        "        ent_coef_end=0.0,\n",
        "        logstd_min=-5.0,\n",
        "        logstd_max=-0.5,\n",
        "\n",
        "        jerk_start_p75=240.0,\n",
        "        jerk_coef_max=0.0010,\n",
        "        jerk_ramp_updates=200,\n",
        "\n",
        "        eval_every_updates=10,\n",
        "        eval_episodes=16,\n",
        "\n",
        "        checkpoint_every_updates=10,\n",
        "\n",
        "        reseed_p_random=0.60,\n",
        "        reseed_p_hard=0.20,\n",
        "        reseed_p_success=0.20,\n",
        "        hard_pool_bottom_frac=0.20,\n",
        "        success_pool_threshold=300.0,\n",
        "    )\n",
        "    trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
