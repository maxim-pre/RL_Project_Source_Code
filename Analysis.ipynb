{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa231b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    %cd \"/content/drive/MyDrive/Python/Bath University/RL1_CW/Louie\"\n",
    "    !pip install swig\n",
    "    !pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262a11728b2b60",
   "metadata": {},
   "source": [
    "# Unpack DDPG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89e09fb87a6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDPG_implementation.Networks import Critic as DDPGCritic, Actor as DDPGActor\n",
    "\n",
    "ddpg_stage1_root = \"DDPG_implementation/outputs/stage 1/\"\n",
    "\n",
    "ddpg_untrained_actor = DDPGActor()\n",
    "ddpg_stage1_actor = DDPGActor()\n",
    "ddpg_stage1_actor.load_state_dict(torch.load(ddpg_stage1_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "ddpg_untrained_critic = DDPGCritic()\n",
    "ddpg_stage1_critic = DDPGCritic()\n",
    "ddpg_stage1_critic.load_state_dict(torch.load(ddpg_stage1_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "with open(ddpg_stage1_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    ddpg_stage1_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(ddpg_stage1_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    ddpg_stage1_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(ddpg_stage1_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    ddpg_stage1_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(ddpg_stage1_root + \"settings.pkl\", \"rb\") as f:\n",
    "    ddpg_stage1_settings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff726d5b5d341e8",
   "metadata": {},
   "source": [
    "# Unpack TD3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ed467a53a74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TD3_implementation.Networks import Critic as TD3Critic, Actor as TD3Actor\n",
    "    \n",
    "\n",
    "td3_stage1_root = \"TD3_implementation/outputs/stage 1/\"\n",
    "\n",
    "td3_untrained_actor = TD3Actor()\n",
    "td3_stage1_actor = TD3Actor()\n",
    "td3_stage1_actor.load_state_dict(torch.load(td3_stage1_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "td3_untrained_critic1 = TD3Critic()\n",
    "td3_stage1_critic1 = TD3Critic()\n",
    "td3_stage1_critic1.load_state_dict(torch.load(td3_stage1_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "td3_untrained_critic2 = TD3Critic()\n",
    "td3_stage1_critic2 = TD3Critic()\n",
    "td3_stage1_critic2.load_state_dict(torch.load(td3_stage1_root + \"critic_network2.pth\", map_location=device))\n",
    "\n",
    "with open(td3_stage1_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    td3_stage1_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage1_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    td3_stage1_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage1_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    td3_stage1_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage1_root + \"settings.pkl\", \"rb\") as f:\n",
    "    td3_stage1_settings = pickle.load(f)\n",
    "    \n",
    "    \n",
    "td3_stage2_root = \"TD3_implementation/outputs/stage 2/\"\n",
    "\n",
    "td3_stage2_actor = TD3Actor()\n",
    "td3_stage2_actor.load_state_dict(torch.load(td3_stage2_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "td3_stage2_critic1 = TD3Critic()\n",
    "td3_stage2_critic1.load_state_dict(torch.load(td3_stage2_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "td3_stage2_critic2 = TD3Critic()\n",
    "td3_stage2_critic2.load_state_dict(torch.load(td3_stage2_root + \"critic_network2.pth\", map_location=device))\n",
    "\n",
    "with open(td3_stage2_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    td3_stage2_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage2_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    td3_stage2_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage2_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    td3_stage2_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage2_root + \"settings.pkl\", \"rb\") as f:\n",
    "    td3_stage2_settings = pickle.load(f)\n",
    "    \n",
    "    \n",
    "td3_stage3_root = \"TD3_implementation/outputs/stage 3/\"\n",
    "\n",
    "td3_stage3_actor = TD3Actor()\n",
    "td3_stage3_actor.load_state_dict(torch.load(td3_stage3_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "td3_stage3_critic1 = TD3Critic()\n",
    "td3_stage3_critic1.load_state_dict(torch.load(td3_stage3_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "td3_stage3_critic2 = TD3Critic()\n",
    "td3_stage3_critic2.load_state_dict(torch.load(td3_stage3_root + \"critic_network2.pth\", map_location=device))\n",
    "\n",
    "with open(td3_stage3_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    td3_stage3_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage3_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    td3_stage3_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage3_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    td3_stage3_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(td3_stage3_root + \"settings.pkl\", \"rb\") as f:\n",
    "    td3_stage3_settings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99353dfb59eb77",
   "metadata": {},
   "source": [
    "# Unpack SAC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127accda9bd44b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC_implementation.Networks import Critic as SACCritic, Actor as SACActor\n",
    "    \n",
    "\n",
    "sac_stage1_root = \"SAC_implementation/outputs/stage 1/\"\n",
    "\n",
    "sac_untrained_actor = SACActor()\n",
    "sac_stage1_actor = SACActor()\n",
    "sac_stage1_actor.load_state_dict(torch.load(sac_stage1_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "sac_untrained_critic1 = SACCritic()\n",
    "sac_stage1_critic1 = SACCritic()\n",
    "sac_stage1_critic1.load_state_dict(torch.load(sac_stage1_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "sac_untrained_critic2 = SACCritic()\n",
    "sac_stage1_critic2 = SACCritic()\n",
    "sac_stage1_critic2.load_state_dict(torch.load(sac_stage1_root + \"critic_network2.pth\", map_location=device))\n",
    "\n",
    "with open(sac_stage1_root + \"episode_alphas.pkl\", \"rb\") as f:\n",
    "    sac_stage1_episode_alphas = pickle.load(f)\n",
    "\n",
    "with open(sac_stage1_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    sac_stage1_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage1_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    sac_stage1_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage1_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    sac_stage1_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage1_root + \"settings.pkl\", \"rb\") as f:\n",
    "    sac_stage1_settings = pickle.load(f)\n",
    "    \n",
    "    \n",
    "sac_stage2_root = \"SAC_implementation/outputs/stage 2/\"\n",
    "\n",
    "sac_stage2_actor = SACActor()\n",
    "sac_stage2_actor.load_state_dict(torch.load(sac_stage2_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "sac_stage2_critic1 = SACCritic()\n",
    "sac_stage2_critic1.load_state_dict(torch.load(sac_stage2_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "sac_stage2_critic2 = SACCritic()\n",
    "sac_stage2_critic2.load_state_dict(torch.load(sac_stage2_root + \"critic_network2.pth\", map_location=device))\n",
    "\n",
    "with open(sac_stage2_root + \"episode_alphas.pkl\", \"rb\") as f:\n",
    "    sac_stage2_episode_alphas = pickle.load(f)\n",
    "\n",
    "with open(sac_stage2_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    sac_stage2_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage2_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    sac_stage2_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage2_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    sac_stage2_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage2_root + \"settings.pkl\", \"rb\") as f:\n",
    "    sac_stage2_settings = pickle.load(f)\n",
    "    \n",
    "    \n",
    "sac_stage3_root = \"SAC_implementation/outputs/stage 3/\"\n",
    "\n",
    "sac_stage3_actor = SACActor()\n",
    "sac_stage3_actor.load_state_dict(torch.load(sac_stage3_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "sac_stage3_critic1 = SACCritic()\n",
    "sac_stage3_critic1.load_state_dict(torch.load(sac_stage3_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "sac_stage3_critic2 = SACCritic()\n",
    "sac_stage3_critic2.load_state_dict(torch.load(sac_stage3_root + \"critic_network2.pth\", map_location=device))\n",
    "\n",
    "with open(sac_stage3_root + \"episode_alphas.pkl\", \"rb\") as f:\n",
    "    sac_stage3_episode_alphas = pickle.load(f)\n",
    "\n",
    "with open(sac_stage3_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    sac_stage3_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage3_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    sac_stage3_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage3_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    sac_stage3_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage3_root + \"settings.pkl\", \"rb\") as f:\n",
    "    sac_stage3_settings = pickle.load(f)\n",
    "    \n",
    "    \n",
    "sac_stage4_root = \"SAC_implementation/outputs/stage 4/\"\n",
    "\n",
    "sac_stage4_actor = SACActor()\n",
    "sac_stage4_actor.load_state_dict(torch.load(sac_stage4_root + \"actor_network.pth\", map_location=device))\n",
    "\n",
    "sac_stage4_critic1 = SACCritic()\n",
    "sac_stage4_critic1.load_state_dict(torch.load(sac_stage4_root + \"critic_network1.pth\", map_location=device))\n",
    "\n",
    "sac_stage4_critic2 = SACCritic()\n",
    "sac_stage4_critic2.load_state_dict(torch.load(sac_stage4_root + \"critic_network2.pth\", map_location=device))\n",
    "\n",
    "with open(sac_stage4_root + \"episode_alphas.pkl\", \"rb\") as f:\n",
    "    sac_stage4_episode_alphas = pickle.load(f)\n",
    "\n",
    "with open(sac_stage4_root + \"episode_rewards.pkl\", \"rb\") as f:\n",
    "    sac_stage4_episode_rewards = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage4_root + \"episode_run_times.pkl\", \"rb\") as f:\n",
    "    sac_stage4_episode_run_times = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage4_root + \"episode_step_counts.pkl\", \"rb\") as f:\n",
    "    sac_stage4_episode_step_counts = pickle.load(f)\n",
    "    \n",
    "with open(sac_stage4_root + \"settings.pkl\", \"rb\") as f:\n",
    "    sac_stage4_settings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43217071a42662f4",
   "metadata": {},
   "source": [
    "# TD3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ddce82deb1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TD3_implementation.Agent import TD3Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f516d89847b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_stage2_agent = TD3Agent(td3_stage2_critic1,\n",
    "                            td3_stage2_critic2,\n",
    "                            td3_stage2_actor,\n",
    "                            device=torch.device(\"cpu\"),\n",
    "                            hardcore=False)\n",
    "\n",
    "td3_stage2_eval_episode_rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    td3_stage2_eval_episode_reward = td3_stage2_agent.test_episode(video=False)\n",
    "    td3_stage2_eval_episode_rewards.append(td3_stage2_eval_episode_reward)\n",
    "\n",
    "print(np.mean(td3_stage2_eval_episode_rewards))\n",
    "print(np.median(td3_stage2_eval_episode_rewards))\n",
    "print(np.min(td3_stage2_eval_episode_rewards))\n",
    "print(np.max(td3_stage2_eval_episode_rewards))\n",
    "print(np.percentile(td3_stage2_eval_episode_rewards, 25))\n",
    "print(np.percentile(td3_stage2_eval_episode_rewards, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8124023ccfd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_stage3_agent = TD3Agent(td3_stage3_critic1,\n",
    "                            td3_stage3_critic2,\n",
    "                            td3_stage3_actor,\n",
    "                            device=torch.device(\"cpu\"),\n",
    "                            hardcore=True)\n",
    "\n",
    "td3_stage3_eval_episode_rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    td3_stage3_eval_episode_reward = td3_stage3_agent.test_episode(video=False)\n",
    "    td3_stage3_eval_episode_rewards.append(td3_stage3_eval_episode_reward)\n",
    "\n",
    "print(np.mean(td3_stage3_eval_episode_rewards))\n",
    "print(np.median(td3_stage3_eval_episode_rewards))\n",
    "print(np.min(td3_stage3_eval_episode_rewards))\n",
    "print(np.max(td3_stage3_eval_episode_rewards))\n",
    "print(np.percentile(td3_stage3_eval_episode_rewards, 25))\n",
    "print(np.percentile(td3_stage3_eval_episode_rewards, 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789560413fb9876",
   "metadata": {},
   "source": [
    "# TD3 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b099aec02925fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_stage1_cum_step_counts = np.array(td3_stage1_episode_step_counts).cumsum()\n",
    "\n",
    "td3_stage2_cum_step_counts = np.array(td3_stage2_episode_step_counts).cumsum()\n",
    "td3_stage1_cum_step_counts_max = td3_stage1_cum_step_counts.max()\n",
    "adj_td3_stage2_cum_step_counts = td3_stage1_cum_step_counts_max + td3_stage2_cum_step_counts\n",
    "\n",
    "td3_stage1_2_cum_step_counts = np.concatenate((td3_stage1_cum_step_counts, adj_td3_stage2_cum_step_counts))\n",
    "td3_stage1_2_episode_rewards = np.concatenate((td3_stage1_episode_rewards, td3_stage2_episode_rewards))\n",
    "\n",
    "def moving_average(x, window):\n",
    "    return np.convolve(x, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "ma_window = 100\n",
    "smoothed_td3_stage1_2_episode_rewards = moving_average(td3_stage1_2_episode_rewards, ma_window)\n",
    "smoothed_td3_stage1_2_cum_step_counts = td3_stage1_2_cum_step_counts[ma_window - 1:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.plot(td3_stage1_2_cum_step_counts, td3_stage1_2_episode_rewards, linewidth=1.0, color=\"blue\", alpha=0.3, label=\"Episode Rewards\")\n",
    "plt.axvline(x=td3_stage1_cum_step_counts_max, linestyle=\"--\", color=\"orange\", linewidth=2.0, label=\"Success\")\n",
    "plt.plot(smoothed_td3_stage1_2_cum_step_counts, smoothed_td3_stage1_2_episode_rewards, linewidth=2.0, color=\"red\", label=\"Moving Average (50 eps)\")\n",
    "plt.xlabel(\"Time Steps\", fontsize=18)\n",
    "plt.ylabel(\"Episode Reward\", fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7555c7ed7515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_stage3_cum_step_counts = np.array(td3_stage3_episode_step_counts).cumsum()\n",
    "\n",
    "def moving_average(x, window):\n",
    "    return np.convolve(x, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "ma_window = 100\n",
    "smoothed_td3_stage3_episode_rewards = moving_average(td3_stage3_episode_rewards, ma_window)\n",
    "smoothed_td3_stage3_cum_step_counts = td3_stage3_cum_step_counts[ma_window - 1:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.plot(td3_stage3_cum_step_counts, td3_stage3_episode_rewards, linewidth=1.0, color=\"blue\", alpha=0.3, label=\"Episode Rewards\")\n",
    "plt.plot(smoothed_td3_stage3_cum_step_counts, smoothed_td3_stage3_episode_rewards, linewidth=2.0, color=\"red\", label=\"Moving Average (50 eps)\")\n",
    "plt.xlabel(\"Time Steps\", fontsize=18)\n",
    "plt.ylabel(\"Episode Reward\", fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64347ddffa1205e0",
   "metadata": {},
   "source": [
    "# SAC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ddbd76341598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC_implementation.Agent import SACAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0fd640b85d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_stage2_agent = SACAgent(sac_stage2_critic1,\n",
    "                            sac_stage2_critic2,\n",
    "                            sac_stage2_actor,\n",
    "                            device=torch.device(\"cpu\"),\n",
    "                            hardcore=False)\n",
    "\n",
    "sac_stage2_eval_episode_rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    sac_stage2_eval_episode_reward = sac_stage2_agent.test_episode(video=False)\n",
    "    sac_stage2_eval_episode_rewards.append(sac_stage2_eval_episode_reward)\n",
    "\n",
    "print(np.mean(sac_stage2_eval_episode_rewards))\n",
    "print(np.median(sac_stage2_eval_episode_rewards))\n",
    "print(np.min(sac_stage2_eval_episode_rewards))\n",
    "print(np.max(sac_stage2_eval_episode_rewards))\n",
    "print(np.percentile(sac_stage2_eval_episode_rewards, 25))\n",
    "print(np.percentile(sac_stage2_eval_episode_rewards, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528729bf682305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_stage4_agent = SACAgent(sac_stage4_critic1,\n",
    "                            sac_stage4_critic2,\n",
    "                            sac_stage4_actor,\n",
    "                            device=torch.device(\"cpu\"),\n",
    "                            hardcore=True)\n",
    "\n",
    "sac_stage4_eval_episode_rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    sac_stage4_eval_episode_reward = sac_stage4_agent.test_episode(video=False)\n",
    "    sac_stage4_eval_episode_rewards.append(sac_stage4_eval_episode_reward)\n",
    "\n",
    "print(np.mean(sac_stage4_eval_episode_rewards))\n",
    "print(np.median(sac_stage4_eval_episode_rewards))\n",
    "print(np.min(sac_stage4_eval_episode_rewards))\n",
    "print(np.max(sac_stage4_eval_episode_rewards))\n",
    "print(np.percentile(sac_stage4_eval_episode_rewards, 25))\n",
    "print(np.percentile(sac_stage4_eval_episode_rewards, 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b9be9d56af69e",
   "metadata": {},
   "source": [
    "# SAC Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7194542effb7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_stage1_cum_step_counts = np.array(sac_stage1_episode_step_counts).cumsum()\n",
    "\n",
    "sac_stage2_cum_step_counts = np.array(sac_stage2_episode_step_counts).cumsum()\n",
    "sac_stage1_cum_step_counts_max = sac_stage1_cum_step_counts.max()\n",
    "adj_sac_stage2_cum_step_counts = sac_stage1_cum_step_counts_max + sac_stage2_cum_step_counts\n",
    "\n",
    "sac_stage1_2_cum_step_counts = np.concatenate((sac_stage1_cum_step_counts, adj_sac_stage2_cum_step_counts))\n",
    "sac_stage1_2_episode_rewards = np.concatenate((sac_stage1_episode_rewards, sac_stage2_episode_rewards))\n",
    "\n",
    "def moving_average(x, window):\n",
    "    return np.convolve(x, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "ma_window = 100\n",
    "smoothed_sac_stage1_2_episode_rewards = moving_average(sac_stage1_2_episode_rewards, ma_window)\n",
    "smoothed_sac_stage1_2_cum_step_counts = sac_stage1_2_cum_step_counts[ma_window - 1:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.plot(sac_stage1_2_cum_step_counts, sac_stage1_2_episode_rewards, linewidth=1.0, color=\"blue\", alpha=0.3, label=\"Episode Rewards\")\n",
    "plt.axvline(x=sac_stage1_cum_step_counts_max, linestyle=\"--\", color=\"orange\", linewidth=2.0, label=\"Success\")\n",
    "plt.plot(smoothed_sac_stage1_2_cum_step_counts, smoothed_sac_stage1_2_episode_rewards, linewidth=2.0, color=\"red\", label=\"Moving Average (50 eps)\")\n",
    "plt.xlabel(\"Time Steps\", fontsize=18)\n",
    "plt.ylabel(\"Episode Reward\", fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4af5229c4d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_stage3_cum_step_counts = np.array(sac_stage3_episode_step_counts).cumsum()\n",
    "\n",
    "sac_stage4_cum_step_counts = np.array(sac_stage4_episode_step_counts).cumsum()\n",
    "sac_stage3_cum_step_counts_max = sac_stage3_cum_step_counts.max()\n",
    "adj_sac_stage4_cum_step_counts = sac_stage3_cum_step_counts_max + sac_stage4_cum_step_counts\n",
    "\n",
    "sac_stage3_4_cum_step_counts = np.concatenate((sac_stage3_cum_step_counts, adj_sac_stage4_cum_step_counts))\n",
    "sac_stage3_4_episode_rewards = np.concatenate((sac_stage3_episode_rewards, sac_stage4_episode_rewards))\n",
    "\n",
    "\n",
    "def moving_average(x, window):\n",
    "    return np.convolve(x, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "ma_window = 100\n",
    "smoothed_sac_stage3_4_episode_rewards = moving_average(sac_stage3_4_episode_rewards, ma_window)\n",
    "smoothed_sac_stage3_4_cum_step_counts = sac_stage3_4_cum_step_counts[ma_window - 1:]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.plot(sac_stage3_4_cum_step_counts, sac_stage3_4_episode_rewards, linewidth=1.0, color=\"blue\", alpha=0.3, label=\"Episode Rewards\")\n",
    "plt.axvline(x=sac_stage3_cum_step_counts_max, linestyle=\"--\", color=\"orange\", linewidth=2.0, label=\"Success\")\n",
    "plt.plot(smoothed_sac_stage3_4_cum_step_counts, smoothed_sac_stage3_4_episode_rewards, linewidth=2.0, color=\"red\", label=\"Moving Average (50 eps)\")\n",
    "plt.xlabel(\"Time Steps\", fontsize=18)\n",
    "plt.ylabel(\"Episode Reward\", fontsize=18)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
